import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from datetime import datetime

# ==========================================
# ì„¤ì •
# ==========================================
URL = "https://zighang.com/company"
OUTPUT_FILE = f"company_list_{datetime.now().strftime('%Y%m%d_%H%M')}.csv"

# ==========================================
# í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤
# ==========================================
class CompanyCrawler:
    def __init__(self):
        chrome_options = Options()
        # chrome_options.add_argument("--headless")  # ì†ë„ ìœ„í•´ í™”ë©´ ë„ì›€ (ìƒíƒœ í™•ì¸ìš©)
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--window-size=1920,1080")
        
        # ë´‡ íƒì§€ ìš°íšŒ
        chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
        
        self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
        self.data = []

    def extract_and_save(self, scroll_count):
        """í˜„ì¬ í™”ë©´ì— ë¡œë”©ëœ ë°ì´í„° ì¶”ì¶œ ë° ì €ì¥"""
        # print(f"   ğŸ’¾ ì¤‘ê°„ ì €ì¥ ì‹œë„ (ìŠ¤í¬ë¡¤ {scroll_count}íšŒ)...")
        
        script = """
        const items = Array.from(document.querySelectorAll('div.hidden.lg\\\\:contents > a'));
        return items.map(item => {
            try {
                const name = item.querySelector('div.typo-body-sm-medium span')?.innerText?.trim() || 'N/A';
                let type = 'N/A';
                const divs = item.querySelectorAll(':scope > div');
                if (divs.length >= 3) {
                    type = divs[2].innerText.trim();
                }
                return {
                    corporate_name: name,
                    industry: type,
                    source_url: item.href
                };
            } catch (e) { return null; }
        }).filter(item => item !== null);
        """
        
        try:
            extracted_data = self.driver.execute_script(script)
        except:
            return

        # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
        current_urls = {d['source_url'] for d in self.data}
        new_count = 0
        for item in extracted_data:
            if item['source_url'] not in current_urls:
                item['crawled_at'] = datetime.now().isoformat()
                self.data.append(item)
                current_urls.add(item['source_url'])
                new_count += 1
        
        if new_count > 0:
            print(f"   âœ¨ {len(self.data)}ê°œ ìˆ˜ì§‘ ì¤‘...")
            self.save_csv()

    def scroll_to_bottom(self):
        """í˜ì´ì§€ ëê¹Œì§€ ìŠ¤í¬ë¡¤í•˜ì—¬ ëª¨ë“  ë°ì´í„° ë¡œë”© (ì´ˆê³ ì† ëª¨ë“œ)"""
        print("ğŸ“œ ìŠ¤í¬ë¡¤ ë‹¤ìš´ ì‹œì‘ (ì´ˆê³ ì† ëª¨ë“œ)...")
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        scroll_count = 0
        
        while True:
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1.0) # 1ì´ˆ ê³ ì • ëŒ€ê¸° (ë¹ ë¥´ê²Œ)
            
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            
            scroll_count += 1
            if scroll_count % 20 == 0: # 20ë²ˆë§ˆë‹¤ ì €ì¥ (ë¹ˆë„ ë‚®ì¶¤)
                print(f"   - ìŠ¤í¬ë¡¤ {scroll_count}íšŒ")
                self.extract_and_save(scroll_count)
            
            if new_height == last_height:
                time.sleep(2)
                new_height = self.driver.execute_script("return document.body.scrollHeight")
                if new_height == last_height:
                    print(f"âœ… ìŠ¤í¬ë¡¤ ì™„ë£Œ (ì´ {scroll_count}íšŒ)")
                    self.extract_and_save(scroll_count)
                    break
            
            last_height = new_height

    def crawl(self):
        print(f"ğŸš€ í¬ë¡¤ë§ ì‹œì‘: {URL}")
        self.driver.get(URL)
        
        try:
            WebDriverWait(self.driver, 20).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "div.hidden.lg\\:contents"))
            )
            print("âœ… í˜ì´ì§€ ë¡œë”© ì™„ë£Œ")
        except:
            print("âŒ ë¡œë”© ì‹¤íŒ¨")
            self.driver.quit()
            return

        self.scroll_to_bottom()
        self.driver.quit()
        print("âœ¨ í¬ë¡¤ë§ ì¢…ë£Œ")

    def save_csv(self):
        if not self.data:
            print("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        df = pd.DataFrame(self.data)
        df.to_csv(OUTPUT_FILE, index=False, encoding="utf-8-sig")
        print(f"ğŸ’¾ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {OUTPUT_FILE}")
        print(df.head())

# ==========================================
# ì‹¤í–‰
# ==========================================
if __name__ == "__main__":
    crawler = CompanyCrawler()
    crawler.crawl()
    crawler.save_csv()
